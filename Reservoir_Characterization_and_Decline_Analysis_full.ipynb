{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reservoir Characterization & Decline Analysis \u2014 Full Pipeline\n",
        "\n",
        "This notebook is a runnable pipeline for:\n",
        "1. Loading synthetic data\n",
        "2. Merging core and well logs (nearest depth)\n",
        "3. Feature engineering\n",
        "4. Training ML models: porosity (regression), permeability (regression, log-target), facies (classification)\n",
        "5. SHAP explanations for model interpretability (optional)\n",
        "6. Decline Curve Analysis (Arps fits per well) and EUR estimation\n",
        "7. Saving trained models and summary outputs\n",
        "\n",
        "Run cells sequentially. Some optional steps (SHAP) will gracefully skip if package not installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Imports & configuration\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score, classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "DATA_DIR = Path('.')\n",
        "OUT_DIR = Path('outputs')\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "print('Working directory:', Path.cwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Load datasets\n",
        "logs = pd.read_csv(DATA_DIR / 'well_logs.csv')\n",
        "core = pd.read_csv(DATA_DIR / 'core_data.csv')\n",
        "prod = pd.read_csv(DATA_DIR / 'production_timeseries.csv')\n",
        "wells = pd.read_csv(DATA_DIR / 'wells_meta.csv')\n",
        "print('logs', logs.shape)\n",
        "print('core', core.shape)\n",
        "print('prod', prod.shape)\n",
        "logs.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Merge core data to nearest log depth\n",
        "We will match each core measurement to the nearest log depth sample for the same well, and extract log curves as features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare a function to find nearest log sample per core depth\n",
        "def merge_core_to_logs(logs_df, core_df, window=0.5):\n",
        "    # logs_df: must contain Well_ID and Depth_m\n",
        "    merged_rows = []\n",
        "    for wid, group in core_df.groupby('Well_ID'):\n",
        "        logs_w = logs_df[logs_df['Well_ID']==wid]\n",
        "        if logs_w.empty:\n",
        "            continue\n",
        "        depths = logs_w['Depth_m'].values\n",
        "        for _, row in group.iterrows():\n",
        "            d = row['Depth_m']\n",
        "            idx = (np.abs(depths - d)).argmin()\n",
        "            log_row = logs_w.iloc[idx]\n",
        "            merged = {**row.to_dict(), **{f'log_{k}': v for k,v in log_row.to_dict().items()}}\n",
        "            merged_rows.append(merged)\n",
        "    return pd.DataFrame(merged_rows)\n",
        "\n",
        "merged = merge_core_to_logs(logs, core)\n",
        "print('merged shape:', merged.shape)\n",
        "merged.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature engineering\n",
        "- Use logs (GR, NPHI, RHOB, DT, RES) and simple derived features\n",
        "- Prepare targets: Porosity, Permeability (log-transform), Facies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = merged.copy()\n",
        "# Extract features from the log_* columns\n",
        "features = ['log_GR','log_NPHI','log_RHOB','log_DT','log_RES']\n",
        "for f in features:\n",
        "    if f not in df.columns:\n",
        "        df[f] = np.nan\n",
        "\n",
        "df['log_GR_over_RHOB'] = df['log_GR'] / (df['log_RHOB'] + 1e-6)\n",
        "df['log_NPHI_times_GR'] = df['log_NPHI'] * df['log_GR']\n",
        "\n",
        "target_por = 'Porosity'\n",
        "target_perm = 'Permeability_mD'\n",
        "target_facies = 'log_Facies' if 'log_Facies' in df.columns else 'Facies'\n",
        "df = df.dropna(subset=features + [target_por, target_perm])\n",
        "print('Final training rows:', df.shape[0])\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Test split and scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df[ ['log_GR','log_NPHI','log_RHOB','log_DT','log_RES','log_GR_over_RHOB','log_NPHI_times_GR'] ].values\n",
        "y_por = df[target_por].values\n",
        "y_perm = df[target_perm].values\n",
        "y_facies = df['Facies'].values if 'Facies' in df.columns else None\n",
        "\n",
        "X_train, X_test, y_por_train, y_por_test = train_test_split(X, y_por, test_size=0.2, random_state=42)\n",
        "_, _, y_perm_train, y_perm_test = train_test_split(X, y_perm, test_size=0.2, random_state=42)\n",
        "if y_facies is not None:\n",
        "    _, _, y_f_train, y_f_test = train_test_split(X, y_facies, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "joblib.dump(scaler, OUT_DIR / 'scaler.joblib')\n",
        "print('Shapes:', X_train_s.shape, X_test_s.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Random Forest for Porosity (regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_por = RandomForestRegressor(n_estimators=150, max_depth=12, random_state=42, n_jobs=-1)\n",
        "rf_por.fit(X_train_s, y_por_train)\n",
        "y_por_pred = rf_por.predict(X_test_s)\n",
        "print('Porosity RMSE:', mean_squared_error(y_por_test, y_por_pred, squared=False))\n",
        "print('Porosity R2:', r2_score(y_por_test, y_por_pred))\n",
        "joblib.dump(rf_por, OUT_DIR / 'rf_porosity.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Random Forest for Permeability (regression on log-target)\n",
        "Permeability is skewed; take log(1+perm) as target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_perm_log = np.log1p(y_perm)\n",
        "y_perm_log_train = np.log1p(y_perm_train)\n",
        "y_perm_log_test = np.log1p(y_perm_test)\n",
        "\n",
        "rf_perm = RandomForestRegressor(n_estimators=200, max_depth=14, random_state=42, n_jobs=-1)\n",
        "rf_perm.fit(X_train_s, y_perm_log_train)\n",
        "y_perm_log_pred = rf_perm.predict(X_test_s)\n",
        "y_perm_pred = np.expm1(y_perm_log_pred)\n",
        "print('Permeability RMSE (mD):', mean_squared_error(y_perm_test, y_perm_pred, squared=False))\n",
        "print('Permeability R2:', r2_score(y_perm_test, y_perm_pred))\n",
        "joblib.dump(rf_perm, OUT_DIR / 'rf_permeability.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Random Forest Classifier for Facies (if available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'Facies' in df.columns:\n",
        "    clf = RandomForestClassifier(n_estimators=200, max_depth=16, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X_train_s, y_f_train)\n",
        "    y_f_pred = clf.predict(X_test_s)\n",
        "    print(classification_report(y_f_test, y_f_pred))\n",
        "    joblib.dump(clf, OUT_DIR / 'rf_facies.joblib')\n",
        "else:\n",
        "    print('Facies column not available in merged dataset.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature importance and SHAP explanations (optional)\n",
        "SHAP can be slow; this cell will attempt to run it and skip if SHAP is not installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = ['GR','NPHI','RHOB','DT','RES','GR_over_RHOB','NPHI_times_GR']\n",
        "try:\n",
        "    import shap\n",
        "    explainer = shap.Explainer(rf_por.predict, X_train_s)\n",
        "    shap_vals = explainer(X_test_s[:200])\n",
        "    # Plot summary (will render inline in Jupyter)\n",
        "    shap.summary_plot(shap_vals, features=X_test_s[:200], feature_names=feature_names)\n",
        "except Exception as e:\n",
        "    print('SHAP not available or failed:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Decline Curve Analysis (Arps) \u2014 functions and per-well fitting\n",
        "We will fit simple Arps (exponential and hyperbolic) to each well's oil rate and select the best by RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def arps_rate(qi, d, b, t):\n",
        "    t = np.asarray(t)\n",
        "    if abs(b) < 1e-8:\n",
        "        return qi * np.exp(-d * t)\n",
        "    return qi / ((1 + b * d * t)**(1.0 / b))\n",
        "\n",
        "def fit_arps(times, rates, initial_guess=(500, 0.05, 0.3)):\n",
        "    # times in months (0,1,2,...), rates in same units as qi\n",
        "    def loss(params):\n",
        "        qi, d, b = params\n",
        "        pred = arps_rate(qi, d, b, times)\n",
        "        return np.sqrt(np.mean((rates - pred)**2))\n",
        "    bounds = [(1e-6, max(rates)*10), (1e-6, 1.0), (0.0, 1.5)]\n",
        "    res = minimize(loss, x0=initial_guess, bounds=bounds, method='L-BFGS-B')\n",
        "    if res.success:\n",
        "        return dict(qi=res.x[0], d=res.x[1], b=res.x[2], rmse=res.fun)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def compute_eur_from_params(qi, d, b, months=6000):\n",
        "    # integrate rate over months to get total volume (monthly units)\n",
        "    t = np.arange(0, months)\n",
        "    rates = arps_rate(qi, d, b, t)\n",
        "    return rates.sum()\n",
        "\n",
        "summary_rows = []\n",
        "for wid, grp in prod.groupby('Well_ID'):\n",
        "    g = grp.sort_values('Date')\n",
        "    g['MonthIdx'] = np.arange(len(g))\n",
        "    times = g['MonthIdx'].values\n",
        "    rates = g['Oil_bopd'].values\n",
        "    if len(rates) < 12 or np.all(rates<=0):\n",
        "        continue\n",
        "    fit = fit_arps(times, rates, initial_guess=(max(rates), 0.03, 0.3))\n",
        "    if fit is None:\n",
        "        continue\n",
        "    eur = compute_eur_from_params(fit['qi'], fit['d'], fit['b'], months=1200)\n",
        "    summary_rows.append({'Well_ID': wid, 'qi': fit['qi'], 'd': fit['d'], 'b': fit['b'], 'rmse': fit['rmse'], 'EUR_bopd_months': eur})\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df.to_csv(OUT_DIR / 'dca_summary.csv', index=False)\n",
        "print('DCA summary rows:', summary_df.shape[0])\n",
        "summary_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save model artifacts and summary\n",
        "Models saved to `outputs/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(rf_por, OUT_DIR / 'rf_porosity.joblib')\n",
        "joblib.dump(rf_perm, OUT_DIR / 'rf_permeability.joblib')\n",
        "if 'clf' in globals():\n",
        "    joblib.dump(clf, OUT_DIR / 'rf_facies.joblib')\n",
        "print('Saved models to', OUT_DIR)\n",
        "summary_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Example: Predict porosity & permeability for all log samples (batch inference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features from logs for batch prediction\n",
        "logs_feat = logs.copy()\n",
        "logs_feat['GR_over_RHOB'] = logs_feat['GR'] / (logs_feat['RHOB'] + 1e-6)\n",
        "logs_feat['NPHI_times_GR'] = logs_feat['NPHI'] * logs_feat['GR']\n",
        "feat_cols = ['GR','NPHI','RHOB','DT','RES','GR_over_RHOB','NPHI_times_GR']\n",
        "X_logs = logs_feat[feat_cols].values\n",
        "X_logs_s = scaler.transform(X_logs)\n",
        "logs_feat['Pred_Porosity'] = rf_por.predict(X_logs_s)\n",
        "logs_feat['Pred_logPerm'] = rf_perm.predict(X_logs_s)\n",
        "logs_feat['Pred_Perm_mD'] = np.expm1(logs_feat['Pred_logPerm'])\n",
        "logs_feat[['Well_ID','Depth_m','Pred_Porosity','Pred_Perm_mD']].head()\n",
        "logs_feat.to_csv(OUT_DIR / 'logs_with_predictions.csv', index=False)\n",
        "print('Wrote logs_with_predictions.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Next steps and tips\n",
        "- Tune hyperparameters with GridSearchCV for better performance.\n",
        "- Add spatial interpolation (Kriging) for field maps.\n",
        "- Add Monte Carlo UQ for DCA parameters to estimate EUR distributions.\n",
        "- Replace synthetic data with actual field data and re-run the pipeline.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}